---
title: "Punto 11 a 14"
author: "Deivid Zhang Figueroa"
date: "24/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(car)
library(perturb)
library(leaps)
library(olsrr)
```


```{r}
#LEER DATOS EN archivo asignado a su grupo, así
library(readr)
datos <- read_csv("archivos/winequality-red.csv")
#datos=read.table(file.choose(),header=T,sep=",",dec=".")
datos1<-datos[1:100,c(1:5,12)]
```


**11.** Ajuste el modelo de regresión sin las observaciones 17 y 34, suponga que se establece que hay un error de digitación con estas dos observaciones, presente sólo la tabla de parámetros ajustados resultante ¿Cambian notoriamente las estimaciones de los parámetros, sus errores estándar y/o la significancia? ¿Qué concluye al respecto? Evalúe el gráfico de normalidad para los residuales estudentizados para este ajuste ¿mejoró la normalidad? Concluya sobre los efectos de este par de observaciones.

**RTA:**

```{r}
# Se elimina las observaciones 17 y 34
datos2 = datos1[-c(17,34),]
attach(datos2)
```

```{r}
datos2 <- datos2 %>% 
  rename(Fija = `fixed acidity`) %>% 
  rename(Volatil = `volatile acidity`) %>% 
  rename(Citrico = `citric acid`) %>% 
  rename(Azucar = `residual sugar`) %>% 
  rename(Cloruros = chlorides) %>% 
  rename(Calidad = quality)
```


```{r}
# Se ajusta el modelo para datos 2 con todas las variables
mod2 = lm(Calidad ~., data = datos2)
# Tabla de parámetros ajustados
summary(mod2)
```

- En la anterior tabla se puede observar que, la estimación de los parámetros $\beta_j$ no presentan cambios notorios, excepto para $\beta_4$, pasa de 0.001826 (signo positivo) considerando las observaciones 17 y 34, a -0.04494 (signo negativo) sin considerar las observaciones 17 y 34. Ahora, para los errores estándar se puede observar que no cambian notoriamente sin considerar las observaciones 17 y 34, sin embargo, para la significancia, si se toma un nivel de significancia de 10%, $\alpha = 0.1$, se tiene que $\beta_1$ es significativo, pues $0.094535 < 0.1$, mientras que considerando las observaciones 17 y 34 esto no pasa, pues $0.14921 > 0.1$.

```{r}
# Gráfico de normalidad para los residuales estudentizados

residualPlots(mod2,tests=FALSE,type="rstudent",quadratic=FALSE,col=2,cex=1.5)
test=shapiro.test(rstudent(mod2)) #Test de normalidad sobre residuales estudentizados
qqnorm(rstudent(mod2),cex=2)

qqline(rstudent(mod2),col=2)
legend("topleft",legend=rbind(c("Statistic W","p.value"),
round(c(test$statistic,test$p.value),digits=5)),cex=1.2)
```

- En la anterior gráfica se puede observar que, sin considerar las observaciones 17 y 34, esto no mejora la normalidad para los residuales estudentizados, pues el valor p es muy pequeño < 0.05, por lo que se rechaza la hipótesis nula $H_0:$ los residuales estudentizados se distribuyen como una normal.

- ¿Cuál sería el efecto de estas 2 observaciones?

**12.** Para el modelo con todas las variables y sin las observaciones 17 y 34, realice diagnósticos de multicolinealidad mediante

```{r}
# borrar esto cuando se unan las partes
miscoeficientes=function(modeloreg,datosreg) {
coefi=coef(modeloreg)
datos2=as.data.frame(scale(datosreg))
coef.std=c(0,coef(lm(update(formula(modeloreg),~.+0),datos2)))
limites=confint(modeloreg,level=0.95)
vifs=c(0,vif(modeloreg))
resul=data.frame("Estimacíon"=coefi,"Límites"=limites,Vif=vifs,Coef.Std=coef.std)
cat("Coeficientes estimados, sus I.C, Vifs y Coeficientes estimados estandarizados","\n")
resul
}
```

**a)** Matriz de correlación de las variables predictoras

**RTA:**

```{r, out.height=6}
cor(datos2)
```

- Matriz de correlaciones: Se detecta una asociación lineal alta entre las variables cítrico y volátil, con un valor de -0.626393071.

**b)** VIF´s

**RTA:**

```{r}
miscoeficientes(mod2,datos2)
```

- Con los valores VIFs: no se observa valores superando la cota de 10. Por este método no se detecta multicolinealidad-

**c)** Proporciones de varianza

```{r}
colldiag(mod2)
```

- Con las proporciones de descomposición de varianza: se puede observar que, en la quinta fila, $\pi_{52}$ y $\pi_{53}$ superan 0.5, y no existe otra fila i donde 2 $\pi_{ij}$ superen esta cota, luego, con estos índices se detecta que volátil y cítrico están involucradas en una relación de multicolinealidad.

**13.** Sin las observaciones 17 y 34, construya modelos de regresión utilizando los métodos de selección (muestre de cada método sólo la tabla de resumen de este y la tabla ANOVA y la de parámetros estimados del modelo finalmente resultante):

```{r}
#Todas las regresiones posibles; da informaci ́on del Cp, R2, R2adj
k=ols_step_all_possible(mod2)
k
plot(k)
```


**a)** Selección según el $R_{adj}^2$

**RTA:** Según el $R_{adj}^2$, los mejores modelos son el 6, 16, 26 y 31, y como estos 3 últimos no muestran un incremento significativo en este estadístico, con respecto al modelo 6, entonces aplicando el principio de parsimonia, se escogería el modelo 6: $$Y_i = \beta_0 + \beta_2X_{i2} + \beta_3X_{i3} + E_i$$,$E_i \overset{\text{iid}}{\sim} N(0,\sigma^2)$.

**b)** Selección según el estadístico $C_p$

**RTA:** teniendo en cuenta que con este estadístico se busca que el modelo con el menor valor $|C_p - p|$, los mejores candidatos son el modelo 6: $|C_p - p| = |4.298177 - 3| = 1.298177$, el modelo 16: $|C_p - p| = |3.804954 - 4| = 0.195046$, el modelo 26: $|C_p - p| = |4.664740 - 5| = 0.33526$ y el modelo 31: $|C_p - p| = |6 - 6| = 0$, pero de acuerdo con la ecuación $$C_p = \frac{SSE_p}{MSE(X_1,X_2,...,X_k)}- (n-2p)$$, esto siempre ocurre con el modelo con todas las variables, por lo tanto, teniendo en cuenta que el modelo 16 tiene el valor más pequeño, entonces por este criterio se selecciona el modelo 16: $$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + E_i$$, $E_i \overset{\text{iid}}{\sim} N(0,\sigma^2)$..

**c)** Stepwise

```{r}
# selección stepwie
ols_step_both_p(mod2,pent = 0.05, prem = 0.05,details=T)
```

- Según el método stepwise, el modelo a usar es el modelo 6: $$Y_i = \beta_0 + \beta_2X_{i2} + \beta_3X_{i3} + E_i$$,$E_i \overset{\text{iid}}{\sim} N(0,\sigma^2)$.

**d)** Selección hacia adelante o forward

```{r}
# selección forward
ols_step_forward_p(mod2,penter=0.05,details = T)
```

- Según el método forward, nuevamente, el modelo seleccionado es el modelo 6.

**e)** Selección hacia atrás o backward

```{r}
# selección backward
ols_step_backward_p(mod2,prem=0.05,details = T)
```

- Según el método backward, nuevamente, el modelo seleccionado es el modelo 6.

**14.** Con base en los anteriores numerales, ¿Cuál modelo sugiere para la variable respuesta? ¿por qué?

**RTA:** 